"""
Multi-Model Vision Ingredient Extractor with Fallback Chain

Priority:
1. Gemini Pro Vision (Google) - Fast, accurate, free tier
2. Qwen-VL (HuggingFace Inference API) - Open source fallback
3. LLaVA (HuggingFace Inference API) - Secondary fallback
4. Tesseract OCR (Local) - Basic text extraction fallback
"""

import os
import base64
from typing import Optional, Dict
from dotenv import load_dotenv
import re

load_dotenv()


class VisionIngredientExtractor:
    """Extract ingredients from images using vision language models."""
    
    def __init__(self):
        self.gemini_api_key = os.getenv("GOOGLE_API_KEY")
        self.hf_api_key = os.getenv("HUGGINGFACE_API_KEY")
        
        # Initialize Gemini
        self.gemini_model = None
        if self.gemini_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=self.gemini_api_key)
                # Use gemini-2.5-flash (latest stable vision model)
                self.gemini_model = genai.GenerativeModel('gemini-2.5-flash')
                print("‚úÖ Gemini Vision initialized (gemini-2.5-flash)")
            except Exception as e:
                print(f"‚ö†Ô∏è Gemini initialization failed: {e}")
    
    async def extract(self, image_bytes: bytes, language: str = "en") -> Dict:
        """
        Extract ingredients with fallback chain.
        
        Returns:
            {
                "ingredients": "sugar, salt, flour",
                "model_used": "gemini" | "qwen" | "llava",
                "confidence": 0.95,
                "success": True
            }
        """
        prompt = self._build_prompt(language)
        
        # Try Gemini first
        result = await self._try_gemini(image_bytes, prompt)
        if result["success"]:
            return result
        
        # Try HuggingFace BLIP (better for ingredient label reading)
        result = await self._try_qwen(image_bytes, prompt)
        if result["success"]:
            return result
        
        # Try HuggingFace ViT-GPT2
        result = await self._try_llava(image_bytes, prompt)
        if result["success"]:
            return result
        
        # Try HuggingFace TrOCR (optical character recognition)
        result = await self._try_trocr(image_bytes)
        if result["success"]:
            return result
        
        # Try Tesseract OCR as final fallback
        result = await self._try_tesseract(image_bytes)
        if result["success"]:
            return result
        
        # All failed - return helpful error message
        print("‚ùå All vision models failed. Please check API keys.")
        return {
            "ingredients": None,
            "model_used": "none",
            "confidence": 0.0,
            "success": False,
            "error": "Unable to extract ingredients. API quota exceeded. Please try again later or use manual entry."
        }
    
    async def _try_gemini(self, image_bytes: bytes, prompt: str) -> Dict:
        """Try Gemini Pro Vision."""
        if not self.gemini_model:
            print("‚ö†Ô∏è Gemini model not initialized")
            return {"success": False}
        
        try:
            import PIL.Image
            import io
            
            # Convert bytes to PIL Image
            image = PIL.Image.open(io.BytesIO(image_bytes))
            
            # Generate content
            response = self.gemini_model.generate_content([prompt, image])
            
            # Check if response has text
            if not response or not hasattr(response, 'text'):
                print("‚ùå Gemini: No text in response")
                return {"success": False}
            
            # Some responses might be blocked for safety
            if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                print(f"‚ö†Ô∏è Gemini blocked: {response.prompt_feedback}")
                return {"success": False}
            
            ingredients = self._clean_response(response.text)
            
            if not ingredients or len(ingredients) < 3:
                print("‚ùå Gemini: Empty or invalid response")
                return {"success": False}
            
            print(f"‚úÖ Gemini extracted: {ingredients[:100]}...")
            
            return {
                "ingredients": ingredients,
                "model_used": "gemini-2.5-flash",
                "confidence": 0.95,
                "success": True
            }
        except Exception as e:
            print(f"‚ùå Gemini failed: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"success": False}
    
    async def _try_qwen(self, image_bytes: bytes, prompt: str) -> Dict:
        """Try Qwen-VL via HuggingFace Inference API."""
        if not self.hf_api_key:
            print("‚ö†Ô∏è No HuggingFace API key, skipping Qwen-VL")
            return {"success": False}
        
        try:
            import httpx
            
            print("üîç Trying Qwen-VL vision model...")
            
            # Encode image to base64
            image_b64 = base64.b64encode(image_bytes).decode()
            
            # HuggingFace Inference API - using a simpler vision model
            api_url = "https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large"
            headers = {"Authorization": f"Bearer {self.hf_api_key}"}
            
            # BLIP uses direct image bytes
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    api_url, 
                    headers=headers, 
                    data=image_bytes
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # BLIP returns description, we need to extract ingredients
                    if isinstance(result, list) and len(result) > 0:
                        caption = result[0].get("generated_text", "")
                        print(f"üìù BLIP caption: {caption}")
                        
                        # Try to extract ingredients from caption
                        # This is a fallback, may not be perfect
                        if any(word in caption.lower() for word in ['ingredient', 'food', 'label']):
                            ingredients = self._clean_response(caption)
                            print(f"‚úÖ BLIP extracted: {ingredients[:100]}...")
                            return {
                                "ingredients": ingredients,
                                "model_used": "blip-hf",
                                "confidence": 0.70,
                                "success": True
                            }
                
            print("‚ö†Ô∏è Qwen-VL: No valid response")
            return {"success": False}
        except Exception as e:
            print(f"‚ùå Qwen-VL failed: {e}")
            return {"success": False}
    
    async def _try_llava(self, image_bytes: bytes, prompt: str) -> Dict:
        """Try ViT GPT2 via HuggingFace Inference API as additional fallback."""
        if not self.hf_api_key:
            print("‚ö†Ô∏è No HuggingFace API key, skipping ViT-GPT2")
            return {"success": False}
        
        try:
            import httpx
            
            print("üîç Trying ViT-GPT2 vision model...")
            
            # Using ViT-GPT2 for image captioning (works better with free tier)
            api_url = "https://api-inference.huggingface.co/models/nlpconnect/vit-gpt2-image-captioning"
            headers = {"Authorization": f"Bearer {self.hf_api_key}"}
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    api_url,
                    headers=headers,
                    data=image_bytes
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    if isinstance(result, list) and len(result) > 0:
                        caption = result[0].get("generated_text", "")
                        print(f"üìù ViT-GPT2 caption: {caption}")
                        
                        # This gives us a general description
                        # We'll pass it through for manual review
                        if caption and len(caption) > 5:
                            ingredients = self._clean_response(caption)
                            print(f"‚úÖ ViT-GPT2 extracted: {ingredients[:100]}...")
                            return {
                                "ingredients": ingredients,
                                "model_used": "vit-gpt2-hf",
                                "confidence": 0.65,
                                "success": True
                            }
            
            print("‚ö†Ô∏è ViT-GPT2: No valid response")
            return {"success": False}
        except Exception as e:
            print(f"‚ùå ViT-GPT2 failed: {e}")
            return {"success": False}
    
    async def _try_trocr(self, image_bytes: bytes) -> Dict:
        """Try TrOCR via HuggingFace for text recognition on ingredient labels."""
        if not self.hf_api_key:
            print("‚ö†Ô∏è No HuggingFace API key, skipping TrOCR")
            return {"success": False}
        
        try:
            import httpx
            from PIL import Image
            import io
            
            print("üîç Trying TrOCR for text extraction...")
            
            # TrOCR works best with preprocessed images
            api_url = "https://api-inference.huggingface.co/models/microsoft/trocr-large-printed"
            headers = {"Authorization": f"Bearer {self.hf_api_key}"}
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    api_url,
                    headers=headers,
                    data=image_bytes
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # TrOCR returns text directly
                    if isinstance(result, list) and len(result) > 0:
                        extracted_text = result[0].get("generated_text", "")
                        print(f"üìù TrOCR text: {extracted_text[:200]}...")
                        
                        # Extract ingredients from the OCR text
                        ingredients = self._extract_ingredients_from_text(extracted_text)
                        
                        if ingredients:
                            print(f"‚úÖ TrOCR extracted: {ingredients[:100]}...")
                            return {
                                "ingredients": ingredients,
                                "model_used": "trocr-hf",
                                "confidence": 0.75,
                                "success": True
                            }
                    elif isinstance(result, dict) and "generated_text" in result:
                        extracted_text = result["generated_text"]
                        ingredients = self._extract_ingredients_from_text(extracted_text)
                        
                        if ingredients:
                            print(f"‚úÖ TrOCR extracted: {ingredients[:100]}...")
                            return {
                                "ingredients": ingredients,
                                "model_used": "trocr-hf",
                                "confidence": 0.75,
                                "success": True
                            }
            
            print("‚ö†Ô∏è TrOCR: No valid text extracted")
            return {"success": False}
        except Exception as e:
            print(f"‚ùå TrOCR failed: {e}")
            return {"success": False}
    
    async def _try_tesseract(self, image_bytes: bytes) -> Dict:
        """Try Tesseract OCR as final fallback."""
        try:
            import PIL.Image
            import io
            import pytesseract
            
            print("üîç Trying Tesseract OCR fallback...")
            
            # Convert bytes to PIL Image
            image = PIL.Image.open(io.BytesIO(image_bytes))
            
            # Extract text using OCR
            text = pytesseract.image_to_string(image)
            
            if not text or len(text.strip()) < 10:
                print("‚ùå OCR: No text extracted")
                return {"success": False}
            
            # Try to find ingredients section
            ingredients_text = self._extract_ingredients_from_text(text)
            
            if ingredients_text and len(ingredients_text) > 10:
                print(f"‚úÖ OCR extracted: {ingredients_text[:100]}...")
                return {
                    "ingredients": ingredients_text,
                    "model_used": "tesseract-ocr",
                    "confidence": 0.60,
                    "success": True
                }
            
            print("‚ùå OCR: Could not find ingredients section")
            return {"success": False}
            
        except ImportError:
            print("‚ö†Ô∏è Tesseract OCR not installed, skipping")
            return {"success": False}
        except Exception as e:
            print(f"‚ùå OCR fallback failed: {e}")
            return {"success": False}
    
    def _extract_ingredients_from_text(self, text: str) -> Optional[str]:
        """Extract ingredients section from OCR text."""
        try:
            # Look for ingredients section
            lines = text.split('\n')
            ingredients_text = ""
            found_section = False
            
            for i, line in enumerate(lines):
                line_lower = line.lower().strip()
                
                # Check if this is the ingredients header
                if any(keyword in line_lower for keyword in ['ingredient', 'ingredients:', 'ingredientes']):
                    found_section = True
                    # Start from next line
                    if ':' in line:
                        # Ingredients might be on same line after colon
                        parts = line.split(':', 1)
                        if len(parts) > 1 and parts[1].strip():
                            ingredients_text = parts[1].strip()
                    continue
                
                # Collect ingredients lines
                if found_section:
                    # Stop at common section headers
                    if any(keyword in line_lower for keyword in [
                        'nutrition', 'allergen', 'contains:', 'may contain', 
                        'manufactured', 'storage', 'direction', 'warning'
                    ]):
                        break
                    
                    # Add non-empty lines
                    if line.strip():
                        ingredients_text += " " + line.strip()
                    else:
                        # Empty line might indicate end of ingredients
                        if ingredients_text:
                            break
            
            # Clean up the extracted text
            ingredients_text = ingredients_text.strip()
            
            # Remove common artifacts
            ingredients_text = re.sub(r'\s+', ' ', ingredients_text)  # Multiple spaces
            ingredients_text = re.sub(r'\s*,\s*', ', ', ingredients_text)  # Normalize commas
            
            return ingredients_text if len(ingredients_text) > 10 else None
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error extracting ingredients from text: {e}")
            return None
    
    async def _try_simple_ocr(self, image_bytes: bytes, language: str) -> Dict:
        """Fallback to simple OCR extraction."""
        try:
            import PIL.Image
            import io
            import pytesseract
            
            # Convert bytes to PIL Image
            image = PIL.Image.open(io.BytesIO(image_bytes))
            
            # Map language codes to Tesseract codes
            lang_map = {
                "hi": "hin", "bn": "ben", "ta": "tam", "te": "tel",
                "mr": "mar", "gu": "guj", "kn": "kan", "ml": "mal",
                "pa": "pan", "en": "eng"
            }
            tesseract_lang = lang_map.get(language, "eng")
            
            # Extract text
            text = pytesseract.image_to_string(image, lang=tesseract_lang)
            
            # Try to find ingredients section
            text_lower = text.lower()
            ingredients_text = ""
            
            # Look for ingredients keyword
            if "ingredients" in text_lower or "ingredient" in text_lower:
                lines = text.split('\n')
                capturing = False
                for line in lines:
                    if "ingredient" in line.lower():
                        capturing = True
                        # Get text after "ingredients:"
                        if ":" in line:
                            ingredients_text = line.split(":", 1)[1].strip()
                        continue
                    if capturing:
                        if line.strip() and not any(stop in line.lower() for stop in ["allergen", "nutrition", "storage", "best before"]):
                            ingredients_text += " " + line.strip()
                        elif line.strip() == "":
                            break
            
            ingredients_text = ingredients_text.strip()
            
            if ingredients_text and len(ingredients_text) > 10:
                print(f"‚úÖ OCR fallback extracted: {ingredients_text[:100]}...")
                return {
                    "ingredients": ingredients_text,
                    "model_used": "tesseract-ocr",
                    "confidence": 0.60,
                    "success": True
                }
            
            print("‚ùå OCR: Could not find ingredients")
            return {"success": False}
        except Exception as e:
            print(f"‚ùå OCR fallback failed: {e}")
            return {"success": False}
    
    def _build_prompt(self, language: str) -> str:
        """Build vision prompt for ingredient extraction."""
        lang_instruction = ""
        if language != "en":
            langs = {
                "hi": "Hindi (‡§π‡§ø‡§®‡•ç‡§¶‡•Ä)", "bn": "Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)", 
                "ta": "Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)", "te": "Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)",
                "mr": "Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)", "gu": "Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)",
                "kn": "Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)", "ml": "Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)", 
                "pa": "Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)"
            }
            lang_instruction = f"\n\nIMPORTANT: Output ingredients in {langs.get(language, 'English')} language using native script."
        
        return f"""Look at this food product image and extract ONLY the ingredient list.

Instructions:
1. Find the "Ingredients:" or similar section on the package
2. Extract all ingredients in the order they appear
3. Return as a comma-separated list
4. Ignore nutrition facts, allergen warnings, barcodes, brand names
5. Fix any text errors or OCR-like mistakes
6. If ingredients are in multiple languages, prefer the clearest one{lang_instruction}

Return ONLY the ingredients as a comma-separated list, nothing else.
Format: ingredient1, ingredient2, ingredient3, ...

Ingredients:"""
    
    def _clean_response(self, text: str) -> str:
        """Clean and normalize model response."""
        text = text.strip()
        
        # Remove common prefixes
        prefixes = [
            "ingredients:", "answer:", "here are the ingredients:",
            "the ingredients are:", "ingredients list:"
        ]
        for prefix in prefixes:
            if text.lower().startswith(prefix):
                text = text[len(prefix):].strip()
        
        # Remove quotes
        if text.startswith('"') and text.endswith('"'):
            text = text[1:-1]
        if text.startswith("'") and text.endswith("'"):
            text = text[1:-1]
        
        # Remove newlines
        text = text.replace("\n", " ").strip()
        
        return text


# Global instance
vision_extractor = VisionIngredientExtractor()
